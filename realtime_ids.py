#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®æ—¶å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ (Real-time Intrusion Detection System)
åŸºäºè”é‚¦å­¦ä¹ æ¨¡å‹çš„å®æ—¶ç½‘ç»œæµé‡æ£€æµ‹æœåŠ¡
"""

import sys
import os
import argparse
import json
import time
import hashlib
import logging
import signal
from datetime import datetime
from collections import deque, defaultdict
from pathlib import Path
from typing import Optional, Dict, List, Tuple

import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F

# å¤ç”¨åŸæ¨¡å—çš„åŠŸèƒ½
import kdd_federated_learning as kfl

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
__version__ = "1.0.0"
VERSION_HASH = hashlib.md5(__version__.encode()).hexdigest()[:8]

# å¼ºåˆ¶ä½¿ç”¨CPUè®¾å¤‡ï¼ˆå³ä½¿æœ‰GPUï¼‰
DEVICE = torch.device("cpu")

# æ—¥å¿—æ ¼å¼é…ç½®
LOG_FORMAT = '%(asctime)s [%(levelname)s] %(message)s'
DATE_FORMAT = '%Y-%m-%d %H:%M:%S'


# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
def setup_logging(log_level: str = "INFO"):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = getattr(logging, log_level.upper(), logging.INFO)
    logging.basicConfig(
        level=level,
        format=LOG_FORMAT,
        datefmt=DATE_FORMAT,
        stream=sys.stderr
    )
    return logging.getLogger(__name__)


logger = setup_logging()


# ============================================================================
# ç»Ÿè®¡å™¨
# ============================================================================
class StatisticsCollector:
    """æ”¶é›†å’Œç»Ÿè®¡æ£€æµ‹æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.latencies = deque(maxlen=window_size)
        self.alert_counts = defaultdict(int)
        self.total_count = 0
        self.start_time = time.time()
        self.last_report_time = time.time()
        
    def record(self, latency_ms: float, threat_label: int, threat_name: str):
        """è®°å½•ä¸€æ¬¡æ£€æµ‹ç»“æœ"""
        self.latencies.append(latency_ms)
        self.total_count += 1
        if threat_label != 0:  # éæ­£å¸¸æµé‡
            self.alert_counts[threat_name] += 1
    
    def get_stats(self) -> Dict:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        if not self.latencies:
            return {
                'qps': 0.0,
                'p50_latency': 0.0,
                'p95_latency': 0.0,
                'p99_latency': 0.0,
                'mean_latency': 0.0,
                'total_count': self.total_count,
                'alert_count': sum(self.alert_counts.values()),
                'alert_breakdown': dict(self.alert_counts),
                'uptime_seconds': time.time() - self.start_time
            }
        
        sorted_latencies = sorted(self.latencies)
        n = len(sorted_latencies)
        
        return {
            'qps': self.total_count / (time.time() - self.start_time) if self.total_count > 0 else 0.0,
            'p50_latency': sorted_latencies[int(n * 0.50)] if n > 0 else 0.0,
            'p95_latency': sorted_latencies[int(n * 0.95)] if n > 0 else 0.0,
            'p99_latency': sorted_latencies[int(n * 0.99)] if n > 0 else 0.0,
            'mean_latency': np.mean(sorted_latencies),
            'total_count': self.total_count,
            'alert_count': sum(self.alert_counts.values()),
            'alert_breakdown': dict(self.alert_counts),
            'uptime_seconds': time.time() - self.start_time
        }
    
    def should_report(self, interval: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š"""
        now = time.time()
        if now - self.last_report_time >= interval:
            self.last_report_time = now
            return True
        return False


# ============================================================================
# æ¨ç†å¼•æ“
# ============================================================================
class InferenceEngine:
    """å®æ—¶æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, batch_size: int = 1):
        self.model_path = model_path
        self.batch_size = batch_size
        self.model = None
        self.max_values = None
        self.input_dim = None
        self.output_dim = None
        self.model_hash = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œç›¸å…³å‚æ•°"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
        
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # è®¡ç®—æ¨¡å‹æ–‡ä»¶å“ˆå¸Œ
        with open(self.model_path, 'rb') as f:
            self.model_hash = hashlib.md5(f.read()).hexdigest()[:16]
        
        # ä½¿ç”¨åŸæ¨¡å—çš„load_modelå‡½æ•°ï¼Œä½†å¼ºåˆ¶ä½¿ç”¨CPU
        checkpoint = torch.load(self.model_path, map_location=DEVICE, weights_only=False)
        
        self.model = kfl.Net2nn(checkpoint['inputs'], checkpoint['outputs'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(DEVICE)  # å¼ºåˆ¶CPU
        self.model.eval()
        
        self.max_values = checkpoint['max_values']
        self.input_dim = checkpoint['inputs']
        self.output_dim = checkpoint['outputs']
        
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: è¾“å…¥ç»´åº¦={self.input_dim}, è¾“å‡ºç±»åˆ«={self.output_dim}")
        logger.info(f"æ¨¡å‹å“ˆå¸Œ: {self.model_hash}")
    
    def preprocess_single(self, raw_data: str, input_format: str = 'auto') -> Optional[torch.Tensor]:
        """
        é¢„å¤„ç†å•æ¡è¾“å…¥æ•°æ®
        
        Args:
            raw_data: åŸå§‹CSVæ ¼å¼å­—ç¬¦ä¸²ï¼ˆé€—å·åˆ†éš”ï¼‰
            input_format: 'csv41', 'csv42', 'csv43', 'auto'
        
        Returns:
            é¢„å¤„ç†åçš„tensorï¼Œå¦‚æœæ ¼å¼é”™è¯¯è¿”å›None
        """
        try:
            # è§£æCSVè¡Œ
            parts = [p.strip() for p in raw_data.split(',')]
            n_cols = len(parts)
            
            # è‡ªåŠ¨æ£€æµ‹æ ¼å¼æˆ–ä½¿ç”¨æŒ‡å®šæ ¼å¼
            if input_format == 'auto':
                if n_cols == 41:
                    # 41åˆ—ï¼šæ— æ ‡ç­¾
                    data = parts
                elif n_cols == 42:
                    # 42åˆ—ï¼šå«threat_type
                    data = parts[:-1]
                elif n_cols == 43:
                    # 43åˆ—ï¼šå«threat_typeå’Œdifficulty
                    data = parts[:-2]
                else:
                    logger.warning(f"æ— æ³•è‡ªåŠ¨è¯†åˆ«æ ¼å¼: {n_cols}åˆ—ï¼ŒæœŸæœ›41/42/43åˆ—")
                    return None
            elif input_format == 'csv41':
                if n_cols != 41:
                    logger.warning(f"æ ¼å¼ä¸åŒ¹é…: æœŸæœ›41åˆ—ï¼Œå®é™…{n_cols}åˆ—")
                    return None
                data = parts
            elif input_format == 'csv42':
                if n_cols != 42:
                    logger.warning(f"æ ¼å¼ä¸åŒ¹é…: æœŸæœ›42åˆ—ï¼Œå®é™…{n_cols}åˆ—")
                    return None
                data = parts[:-1]  # ç§»é™¤threat_type
            elif input_format == 'csv43':
                if n_cols != 43:
                    logger.warning(f"æ ¼å¼ä¸åŒ¹é…: æœŸæœ›43åˆ—ï¼Œå®é™…{n_cols}åˆ—")
                    return None
                data = parts[:-2]  # ç§»é™¤threat_typeå’Œdifficulty
            else:
                logger.error(f"æœªçŸ¥çš„è¾“å…¥æ ¼å¼: {input_format}")
                return None
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆå•è¡Œï¼‰
            df = pd.DataFrame([data], columns=kfl.colnames[:-1])  # ä¸åŒ…æ‹¬threat_type
            
            # æ·»åŠ è™šæ‹Ÿçš„threat_typeåˆ—ï¼ˆç”¨äºå…¼å®¹é¢„å¤„ç†å‡½æ•°ï¼‰
            df['threat_type'] = 0
            
            # ä½¿ç”¨åŸæ¨¡å—çš„é¢„å¤„ç†å‡½æ•°
            x_data = kfl.preprocess_new_data(df, self.max_values, kfl.colnames)
            
            # è½¬æ¢ä¸ºtensorï¼ˆå•æ ·æœ¬ï¼‰
            x_tensor = torch.tensor(x_data, dtype=torch.float32).to(DEVICE)
            
            return x_tensor
            
        except Exception as e:
            logger.error(f"é¢„å¤„ç†å¤±è´¥: {e}, æ•°æ®: {raw_data[:100]}...")
            return None
    
    def infer(self, x_tensor: torch.Tensor) -> Tuple[int, float, np.ndarray]:
        """
        æ‰§è¡Œæ¨ç†
        
        Returns:
            (é¢„æµ‹æ ‡ç­¾, ç½®ä¿¡åº¦, æ‰€æœ‰ç±»åˆ«æ¦‚ç‡)
        """
        with torch.no_grad():
            output = self.model(x_tensor)
            probs = F.softmax(output, dim=-1)
            pred_label = output.argmax(dim=-1).item()
            confidence = probs[0][pred_label].item()
            probs_array = probs[0].cpu().numpy()
        
        return pred_label, confidence, probs_array


# ============================================================================
# è¾“å…¥è¯»å–å™¨
# ============================================================================
class InputReader:
    """è¾“å…¥æ•°æ®è¯»å–å™¨ï¼ˆæ”¯æŒstdinå’Œtail-fileï¼‰"""
    
    def __init__(self, source_type: str, source_path: Optional[str] = None):
        self.source_type = source_type
        self.source_path = source_path
        self.file_handle = None
        self._setup()
    
    def _setup(self):
        """è®¾ç½®è¾“å…¥æº"""
        if self.source_type == 'stdin':
            self.file_handle = sys.stdin
            logger.info("è¾“å…¥æº: æ ‡å‡†è¾“å…¥ (stdin)")
        elif self.source_type == 'tail-file':
            if not self.source_path:
                raise ValueError("tail-fileæ¨¡å¼éœ€è¦æŒ‡å®šæ–‡ä»¶è·¯å¾„")
            if not os.path.exists(self.source_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {self.source_path}")
            self.file_handle = open(self.source_path, 'r', encoding='utf-8')
            # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾ï¼ˆåªè¯»å–æ–°è¿½åŠ çš„å†…å®¹ï¼‰
            self.file_handle.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            logger.info(f"è¾“å…¥æº: æ–‡ä»¶å°¾ç›‘å¬ (tail-file: {self.source_path})")
        else:
            raise ValueError(f"æœªçŸ¥çš„è¾“å…¥æºç±»å‹: {self.source_type}")
    
    def read_line(self, timeout: float = 0.1) -> Optional[str]:
        """è¯»å–ä¸€è¡Œæ•°æ®ï¼ˆéé˜»å¡ï¼‰"""
        try:
            if self.source_type == 'stdin':
                # stdinå¯èƒ½æœ‰ç¼“å†²ï¼Œä½¿ç”¨readline
                line = self.file_handle.readline()
                if not line:
                    return None
                return line.strip()
            elif self.source_type == 'tail-file':
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ–°å†…å®¹
                line = self.file_handle.readline()
                if not line:
                    return None
                return line.strip()
        except Exception as e:
            logger.error(f"è¯»å–è¾“å…¥å¤±è´¥: {e}")
            return None
    
    def close(self):
        """å…³é—­è¾“å…¥æº"""
        if self.file_handle and self.source_type == 'tail-file':
            self.file_handle.close()


# ============================================================================
# è¾“å‡ºå™¨
# ============================================================================
class OutputWriter:
    """ç»“æœè¾“å‡ºå™¨ï¼ˆæ”¯æŒstdoutå’Œæ–‡ä»¶ï¼‰"""
    
    def __init__(self, output_path: Optional[str] = None, output_format: str = 'jsonl'):
        self.output_path = output_path
        self.output_format = output_format
        self.file_handle = None
        
        if output_path:
            self.file_handle = open(output_path, 'w', encoding='utf-8')
            logger.info(f"è¾“å‡ºç›®æ ‡: æ–‡ä»¶ ({output_path})")
        else:
            self.file_handle = sys.stdout
            logger.info("è¾“å‡ºç›®æ ‡: æ ‡å‡†è¾“å‡º (stdout)")
    
    def write_result(self, result: Dict):
        """å†™å…¥ä¸€æ¡æ£€æµ‹ç»“æœ"""
        if self.output_format == 'jsonl':
            json_str = json.dumps(result, ensure_ascii=False)
            self.file_handle.write(json_str + '\n')
            self.file_handle.flush()
        elif self.output_format == 'csv':
            # CSVæ ¼å¼ï¼štimestamp,src,threat_type,confidence,latency_ms
            csv_line = f"{result['timestamp']},{result.get('src', 'NA')},{result['threat_type']},{result['confidence']:.4f},{result['latency_ms']:.2f}\n"
            self.file_handle.write(csv_line)
            self.file_handle.flush()
    
    def close(self):
        """å…³é—­è¾“å‡ºæ–‡ä»¶"""
        if self.output_path and self.file_handle:
            self.file_handle.close()


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def health_check(model_path: str):
    """å¥åº·æ£€æŸ¥ï¼šæ‰“å°æ¨¡å‹ä¿¡æ¯åé€€å‡º"""
    try:
        engine = InferenceEngine(model_path, batch_size=1)
        print(f"\n{'='*80}")
        print("å®æ—¶å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ - å¥åº·æ£€æŸ¥")
        print(f"{'='*80}")
        print(f"ç‰ˆæœ¬: {__version__} (hash: {VERSION_HASH})")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ¨¡å‹å“ˆå¸Œ: {engine.model_hash}")
        print(f"è¾“å…¥ç»´åº¦: {engine.input_dim}")
        print(f"è¾“å‡ºç±»åˆ«æ•°: {engine.output_dim}")
        print(f"è¿è¡Œè®¾å¤‡: {DEVICE}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"{'='*80}\n")
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç³»ç»Ÿå°±ç»ª")
        sys.exit(0)
    except Exception as e:
        print(f"âœ— å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        sys.exit(1)


def parse_single_line(line: str, input_format: str) -> Tuple[Optional[str], Optional[str]]:
    """
    è§£æè¾“å…¥è¡Œï¼Œæå–ç‰¹å¾æ•°æ®å’ŒæºIPï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    
    Returns:
        (ç‰¹å¾æ•°æ®CSVå­—ç¬¦ä¸², æºIPæˆ–None)
    """
    line = line.strip()
    if not line or line.startswith('#'):
        return None, None
    
    parts = [p.strip() for p in line.split(',')]
    
    # å°è¯•æå–æºIPï¼ˆé€šå¸¸åœ¨å‰å‡ åˆ—ï¼Œä½†NSL-KDDæ ¼å¼ä¸­æ²¡æœ‰æ ‡å‡†IPå­—æ®µï¼‰
    # è¿™é‡Œå‡è®¾å¦‚æœæœ‰IPæ ¼å¼çš„æ•°æ®ï¼Œå¯èƒ½åœ¨æŸä¸ªä½ç½®
    src = None
    for part in parts[:5]:  # æ£€æŸ¥å‰5åˆ—
        if '.' in part and part.count('.') == 3:
            try:
                # éªŒè¯æ˜¯å¦ä¸ºIPåœ°å€æ ¼å¼
                octets = part.split('.')
                if all(0 <= int(o) <= 255 for o in octets):
                    src = part
                    break
            except:
                pass
    
    return line, src


def main():
    parser = argparse.ArgumentParser(
        description='å®æ—¶å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ - åŸºäºè”é‚¦å­¦ä¹ çš„å®æ—¶ç½‘ç»œæµé‡æ£€æµ‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹å‘½ä»¤:
  # ä»stdinè¯»å–
  cat data.csv | python realtime_ids.py --stdin --model-path ckpt/model.pth
  
  # ç›‘å¬æ–‡ä»¶è¿½åŠ 
  python realtime_ids.py --tail-file /var/log/network.log --model-path ckpt/model.pth --output results.jsonl
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python realtime_ids.py --stdin --model-path ckpt/model.pth --output results.csv --format csv
        """
    )
    
    # è¾“å…¥æºï¼ˆäºŒé€‰ä¸€ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--stdin', action='store_true',
                            help='ä»æ ‡å‡†è¾“å…¥è¯»å–ï¼ˆç®¡é“/SSH/agentä¸ŠæŠ¥ï¼‰')
    input_group.add_argument('--tail-file', type=str, metavar='PATH',
                            help='æŒç»­ç›‘å¬æ–‡ä»¶å°¾éƒ¨è¿½åŠ ï¼ˆtail -fæ¨¡å¼ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model-path', type=str, default='ckpt/model.pth',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ckpt/model.pth)')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--batch-size', type=int, default=1,
                       help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1ï¼Œå®æ—¶åœºæ™¯å»ºè®®ä¸º1)')
    
    # è¾“å…¥æ ¼å¼
    parser.add_argument('--format', type=str, default='auto',
                       choices=['auto', 'csv41', 'csv42', 'csv43'],
                       help='è¾“å…¥æ•°æ®æ ¼å¼ (é»˜è®¤: autoè‡ªåŠ¨æ£€æµ‹)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: stdout)')
    parser.add_argument('--output-format', type=str, default='jsonl',
                       choices=['jsonl', 'csv'],
                       help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: jsonl)')
    
    # æ—¥å¿—å’Œç»Ÿè®¡
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    parser.add_argument('--stats-interval', type=float, default=10.0,
                       help='ç»Ÿè®¡æŠ¥å‘Šé—´éš”ï¼ˆç§’ï¼‰(é»˜è®¤: 10.0)')
    
    # ç‰¹æ®Šæ¨¡å¼
    parser.add_argument('--dry-run', action='store_true',
                       help='å¹²è¿è¡Œæ¨¡å¼ï¼šåªè§£æä¸æ¨ç†ï¼ˆç”¨äºæ’æŸ¥è¾“å…¥æ ¼å¼ï¼‰')
    parser.add_argument('--health-check', action='store_true',
                       help='å¥åº·æ£€æŸ¥ï¼šæ‰“å°æ¨¡å‹ä¿¡æ¯åé€€å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    global logger
    logger = setup_logging(args.log_level)
    
    # å¥åº·æ£€æŸ¥æ¨¡å¼
    if args.health_check:
        health_check(args.model_path)
    
    # åˆå§‹åŒ–ç»„ä»¶
    try:
        # åŠ è½½æ¨¡å‹
        engine = InferenceEngine(args.model_path, batch_size=args.batch_size)
        
        # è®¾ç½®è¾“å…¥æº
        source_type = 'stdin' if args.stdin else 'tail-file'
        source_path = None if args.stdin else args.tail_file
        reader = InputReader(source_type, source_path)
        
        # è®¾ç½®è¾“å‡º
        writer = OutputWriter(args.output, args.output_format)
        
        # ç»Ÿè®¡å™¨
        stats = StatisticsCollector(window_size=1000)
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # å¯åŠ¨ä¿¡æ¯
    logger.info("=" * 80)
    logger.info("å®æ—¶å…¥ä¾µæ£€æµ‹ç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 80)
    logger.info(f"ç‰ˆæœ¬: {__version__} (hash: {VERSION_HASH})")
    logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"æ¨¡å‹å“ˆå¸Œ: {engine.model_hash}")
    logger.info(f"è¾“å…¥ç»´åº¦: {engine.input_dim}")
    logger.info(f"è¾“å‡ºç±»åˆ«æ•°: {engine.output_dim}")
    logger.info(f"è¿è¡Œè®¾å¤‡: {DEVICE} (å¼ºåˆ¶CPUæ¨¡å¼)")
    logger.info(f"è¾“å…¥æ¨¡å¼: {source_type}")
    if source_path:
        logger.info(f"ç›‘å¬æ–‡ä»¶: {source_path}")
    logger.info(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    logger.info(f"è¾“å…¥æ ¼å¼: {args.format}")
    logger.info(f"è¾“å‡ºæ ¼å¼: {args.output_format}")
    logger.info(f"ç»Ÿè®¡é—´éš”: {args.stats_interval}ç§’")
    logger.info("=" * 80)
    logger.info("å¼€å§‹å®æ—¶æ£€æµ‹...")
    logger.info("")
    
    # ä¿¡å·å¤„ç†ï¼ˆä¼˜é›…é€€å‡ºï¼‰
    def signal_handler(sig, frame):
        logger.info("\næ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        stats_final = stats.get_stats()
        logger.info("=" * 80)
        logger.info("æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)
        logger.info(f"æ€»æ£€æµ‹æ•°: {stats_final['total_count']}")
        logger.info(f"æ€»å‘Šè­¦æ•°: {stats_final['alert_count']}")
        logger.info(f"å¹³å‡QPS: {stats_final['qps']:.2f}")
        logger.info(f"å¹³å‡å»¶è¿Ÿ: {stats_final['mean_latency']:.2f}ms")
        logger.info(f"P95å»¶è¿Ÿ: {stats_final['p95_latency']:.2f}ms")
        logger.info(f"è¿è¡Œæ—¶é•¿: {stats_final['uptime_seconds']:.1f}ç§’")
        logger.info("å‘Šè­¦åˆ†ç±»ç»Ÿè®¡:")
        for threat_type, count in stats_final['alert_breakdown'].items():
            logger.info(f"  {threat_type}: {count}")
        logger.info("=" * 80)
        reader.close()
        writer.close()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ä¸»å¾ªç¯
    try:
        while True:
            # è¯»å–è¾“å…¥
            line = reader.read_line()
            if line is None:
                time.sleep(0.01)  # é¿å…CPUç©ºè½¬
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºç»Ÿè®¡
                if stats.should_report(args.stats_interval):
                    stats_data = stats.get_stats()
                    if stats_data['total_count'] > 0:
                        logger.info("=" * 80)
                        logger.info(f"ç»Ÿè®¡æŠ¥å‘Š [æ€»æ£€æµ‹: {stats_data['total_count']}, å‘Šè­¦: {stats_data['alert_count']}]")
                        logger.info(f"QPS: {stats_data['qps']:.2f} | "
                                  f"å»¶è¿Ÿ: å‡å€¼={stats_data['mean_latency']:.2f}ms, "
                                  f"P50={stats_data['p50_latency']:.2f}ms, "
                                  f"P95={stats_data['p95_latency']:.2f}ms, "
                                  f"P99={stats_data['p99_latency']:.2f}ms")
                        if stats_data['alert_breakdown']:
                            logger.info("å‘Šè­¦åˆ†ç±»:")
                            for threat_type, count in stats_data['alert_breakdown'].items():
                                logger.info(f"  {threat_type}: {count}")
                        logger.info("=" * 80)
                continue
            
            # è§£æè¾“å…¥
            raw_data, src = parse_single_line(line, args.format)
            if raw_data is None:
                continue
            
            # å¹²è¿è¡Œæ¨¡å¼
            if args.dry_run:
                logger.info(f"[DRY-RUN] è§£ææˆåŠŸ: {len(raw_data.split(','))}åˆ—, æº: {src or 'NA'}")
                continue
            
            # é¢„å¤„ç†
            start_time = time.time()
            x_tensor = engine.preprocess_single(raw_data, args.format)
            if x_tensor is None:
                continue
            
            # æ¨ç†
            try:
                pred_label, confidence, probs = engine.infer(x_tensor)
                latency_ms = (time.time() - start_time) * 1000
                
                # è·å–å¨èƒç±»å‹åç§°
                threat_name = kfl.THREAT_LABELS.get(pred_label, f'æœªçŸ¥({pred_label})')
                
                # è®°å½•ç»Ÿè®¡
                stats.record(latency_ms, pred_label, threat_name)
                
                # æ„å»ºç»“æœ
                result = {
                    'timestamp': datetime.now().isoformat(),
                    'src': src or 'NA',
                    'threat_type': threat_name,
                    'threat_label': int(pred_label),
                    'confidence': float(confidence),
                    'latency_ms': float(latency_ms),
                    'model_version': engine.model_hash
                }
                
                # è¾“å‡ºç»“æœ
                writer.write_result(result)
                
                # å¦‚æœæ˜¯å‘Šè­¦ï¼Œåœ¨stderrè¾“å‡ºï¼ˆä¾¿äºåŒºåˆ†ï¼‰
                if pred_label != 0:
                    logger.warning(f"ğŸš¨ å‘Šè­¦: {threat_name} (ç½®ä¿¡åº¦: {confidence:.2%}, å»¶è¿Ÿ: {latency_ms:.2f}ms, æº: {src or 'NA'})")
                
            except Exception as e:
                logger.error(f"æ¨ç†å¤±è´¥: {e}, æ•°æ®: {raw_data[:100]}...")
                continue
            
    except KeyboardInterrupt:
        signal_handler(None, None)
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶é”™è¯¯: {e}", exc_info=True)
        reader.close()
        writer.close()
        sys.exit(1)


if __name__ == '__main__':
    main()

