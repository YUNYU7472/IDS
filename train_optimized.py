#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆæœ¬çš„è”é‚¦å­¦ä¹ å…¥ä¾µæ£€æµ‹ç³»ç»Ÿè®­ç»ƒè„šæœ¬
Performance-Optimized Training Script for Intrusion Detection
åŸºäºkdd-hlé…ç½®ï¼ˆå‡†ç¡®ç‡å¯è¾¾97.5%ï¼‰+ GPUåŠ é€Ÿ
"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import random
import time
import json
from datetime import datetime
from tqdm import tqdm

import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from imblearn.over_sampling import SMOTE
import copy

pd.options.display.float_format = "{:,.4f}".format

# ============================================================================
# GPU Configuration (è‡ªåŠ¨ä½¿ç”¨æœ€ä¼˜è®¾å¤‡)
# ============================================================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("=" * 80)
print("ğŸš€ æ€§èƒ½ä¼˜åŒ–ç‰ˆè®­ç»ƒè„šæœ¬å¯åŠ¨")
print("=" * 80)
print(f"ğŸ“Š ä½¿ç”¨è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"ğŸ® GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    # å¯ç”¨cudnnè‡ªåŠ¨ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    print("âš¡ CuDNNè‡ªåŠ¨ä¼˜åŒ–: å·²å¯ç”¨")
print("=" * 80)

# ============================================================================
# æœ€ä¼˜é…ç½®å‚æ•°ï¼ˆåŸºäºå®éªŒç»“æœï¼‰
# ============================================================================
THREAT_TYPE = 'threat_type'

# è®­ç»ƒè¶…å‚æ•° (æœ€ä¼˜é…ç½® - è¯·å‹¿éšæ„ä¿®æ”¹)
learning_rate = 0.01        # å­¦ä¹ ç‡
numEpoch = 20               # è®­ç»ƒè½®æ•°
batch_size = 32             # æ‰¹æ¬¡å¤§å° âš ï¸ æœ€ä¼˜å€¼ï¼Œä¸è¦æ”¹æˆ64
momentum = 0.9              # SGDåŠ¨é‡
print_amount = 3            # æ¯ä¸ªepochæ‰“å°æ¬¡æ•°
number_of_slices = 2        # è”é‚¦å­¦ä¹ èŠ‚ç‚¹æ•°ï¼ˆ2ä¸ªèŠ‚ç‚¹è¡¨ç°æœ€å¥½ï¼‰
isSmote = True              # ä½¿ç”¨SMOTEæ•°æ®å¹³è¡¡ï¼ˆæ˜¾è‘—æå‡æ€§èƒ½ï¼‰

# æ¨¡å‹ä¿å­˜é…ç½®
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_save_path = f"best_model_{timestamp}.pth"
results_save_path = f"training_results_{timestamp}.json"

print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
print(f"   - å­¦ä¹ ç‡: {learning_rate}")
print(f"   - è®­ç»ƒè½®æ•°: {numEpoch}")
print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"   - è”é‚¦èŠ‚ç‚¹æ•°: {number_of_slices}")
print(f"   - SMOTEæ•°æ®å¹³è¡¡: {'âœ“ å¯ç”¨' if isSmote else 'âœ— ç¦ç”¨'}")
print(f"   - æ¨¡å‹ä¿å­˜è·¯å¾„: {model_save_path}")
print("=" * 80)

data_path = "./data/"

# æ•°æ®åˆ—å®šä¹‰
colnames = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',
            'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',
            'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',
            'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',
            'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',
            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',
            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',
            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
            'dst_host_srv_rerror_rate', 'threat_type']

# ============================================================================
# ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰
# ============================================================================
class Net2nn(nn.Module):
    """3å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ (ä¼˜åŒ–ç‰ˆæœ¬)"""
    def __init__(self, inputs, outputs):
        super(Net2nn, self).__init__()
        self.fc1 = nn.Linear(inputs, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, outputs)
        
        # æƒé‡åˆå§‹åŒ– (Xavieråˆå§‹åŒ–ï¼Œæå‡æ€§èƒ½)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.xavier_uniform_(self.fc3.weight)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ============================================================================
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================
print("\nğŸ“‚ åŠ è½½NSL-KDDæ•°æ®é›†...")
start_time = time.time()

df_train = pd.read_csv(data_path + "KDDTrain+.csv", header=None)
df_train = df_train.iloc[:, :-1]

df_test = pd.read_csv(data_path + "KDDTest+.csv", header=None)
df_test = df_test.iloc[:, :-1]

df_train.columns = colnames
df_test.columns = colnames

print(f"   âœ“ è®­ç»ƒé›†: {df_train.shape[0]} æ ·æœ¬")
print(f"   âœ“ æµ‹è¯•é›†: {df_test.shape[0]} æ ·æœ¬")

# å¨èƒç±»å‹ç¼–ç  (è®­ç»ƒé›†)
print("\nğŸ”„ ç¼–ç å¨èƒç±»å‹...")
threat_mapping = {
    'normal': 0,
    'back': 1, 'land': 1, 'neptune': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,  # DoS
    'buffer_overflow': 2, 'loadmodule': 2, 'perl': 2, 'rootkit': 2,  # U2R
    'ftp_write': 3, 'guess_passwd': 3, 'imap': 3, 'multihop': 3, 'phf': 3,  # R2L
    'spy': 3, 'warezclient': 3, 'warezmaster': 3,
    'ipsweep': 4, 'nmap': 4, 'portsweep': 4, 'satan': 4,  # Probe
    'unknown': 6
}

for threat, code in tqdm(threat_mapping.items(), desc='ç¼–ç è®­ç»ƒé›†å¨èƒç±»å‹', ncols=80):
    df_train.loc[df_train['threat_type'] == threat, 'threat_type'] = code
    df_test.loc[df_test['threat_type'] == threat, 'threat_type'] = code

# æµ‹è¯•é›†é¢å¤–å¨èƒç±»å‹
test_extra_mapping = {
    'apache2': 1, 'processtable': 1, 'mailbomb': 1, 'udpstorm': 1, 'worm': 1,
    'ps': 2, 'xterm': 2, 'sqlattack': 2,
    'snmpgetattack': 3, 'httptunnel': 3, 'snmpguess': 3, 'named': 3,
    'sendmail': 3, 'xlock': 3, 'xsnoop': 3,
    'mscan': 4, 'saint': 4
}

for threat, code in tqdm(test_extra_mapping.items(), desc='ç¼–ç æµ‹è¯•é›†é¢å¤–ç±»å‹', ncols=80):
    df_test.loc[df_test['threat_type'] == threat, 'threat_type'] = code

# åˆå¹¶æ•°æ®é›†
df_full = pd.concat([df_train, df_test])

# ç¡®ä¿å¨èƒç±»å‹åˆ—ä¸ºæ•°å€¼ç±»å‹ï¼Œå°†ä»»ä½•éæ•°å€¼æ ‡ç­¾è½¬æ¢ä¸º0ï¼ˆæ­£å¸¸ï¼‰
df_full[THREAT_TYPE] = pd.to_numeric(df_full[THREAT_TYPE], errors='coerce').fillna(0).astype(int)

# é«˜çº§å¨èƒåˆ†ç±» (0=æ­£å¸¸, 1=æ”»å‡»)
df_full.loc[(df_full[THREAT_TYPE] != 0), THREAT_TYPE] = 1
print(f"\nğŸ“Š å¨èƒç±»å‹åˆ†å¸ƒ:")
print(df_full[THREAT_TYPE].value_counts())

# æ•°æ®å½’ä¸€åŒ–
print(f"\nğŸ”„ æ•°æ®å½’ä¸€åŒ–å¤„ç†...")
threat_type_df = df_full['threat_type'].copy()

# éªŒè¯å¹¶æ¸…ç†å¨èƒç±»å‹æ•°æ®
print(f"   å¨èƒç±»å‹å”¯ä¸€å€¼: {sorted(threat_type_df.unique())}")
if threat_type_df.isna().sum() > 0:
    print(f"   âš ï¸ å‘ç° {threat_type_df.isna().sum()} ä¸ªNaNå€¼ï¼Œå·²å¡«å……ä¸º0")
    threat_type_df = threat_type_df.fillna(0)
threat_type_df = threat_type_df.astype(int)

numerical_columns = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot',
                     'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',
                     'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'count',
                     'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',
                     'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',
                     'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',
                     'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
                     'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']

numerical_df_full = df_full[numerical_columns].copy()
numerical_df_full = numerical_df_full.loc[:, (numerical_df_full != numerical_df_full.iloc[0]).any()]

# ä¿å­˜æœ€å¤§å€¼ç”¨äºåç»­å½’ä¸€åŒ–
max_values = numerical_df_full.max()

# å½’ä¸€åŒ–åˆ°[0,1]
final_df_full = numerical_df_full / numerical_df_full.max()
df_normalized = pd.concat([final_df_full, threat_type_df], axis=1)

print(f"   âœ“ å½’ä¸€åŒ–åæ•°æ®ç»´åº¦: {df_normalized.shape}")
print(f"â±ï¸  æ•°æ®åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def divide_train_test(df, proportion=0.1):
    """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä¿æŒç±»åˆ«æ··åˆï¼‰"""
    # æŒ‰ç±»åˆ«åˆ†å±‚é‡‡æ ·ï¼Œç„¶ååˆå¹¶
    df_test_parts = []
    df_train_parts = []
    
    for key in df[THREAT_TYPE].unique():
        df_part = df[df[THREAT_TYPE] == key].copy()
        split_idx = int(df_part.shape[0] * proportion)
        df_test_parts.append(df_part.iloc[:split_idx])
        df_train_parts.append(df_part.iloc[split_idx:])
    
    # åˆå¹¶å¹¶æ‰“ä¹±
    df_test = pd.concat(df_test_parts).sample(frac=1, random_state=42).reset_index(drop=True)
    df_train = pd.concat(df_train_parts).sample(frac=1, random_state=42).reset_index(drop=True)
    
    return df_train, df_test

def get_data_for_slices(df_train, number_of_slices, isSmote=False):
    """ä¸ºæ¯ä¸ªè”é‚¦èŠ‚ç‚¹å‡†å¤‡æ•°æ®ï¼ˆç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰æ··åˆç±»åˆ«ï¼‰"""
    x_data_dict = dict()
    y_data_dict = dict()
    
    # å°†è®­ç»ƒæ•°æ®å¹³å‡åˆ†é…ç»™å„ä¸ªèŠ‚ç‚¹
    total_samples = len(df_train)
    samples_per_slice = total_samples // number_of_slices
    
    for i in range(number_of_slices):
        xname = "x_train" + str(i)
        yname = "y_train" + str(i)
        
        # æ¯ä¸ªèŠ‚ç‚¹è·å–ä¸€éƒ¨åˆ†æ•°æ®
        start_idx = i * samples_per_slice
        if i == number_of_slices - 1:
            # æœ€åä¸€ä¸ªèŠ‚ç‚¹è·å–å‰©ä½™æ‰€æœ‰æ•°æ®
            df_part = df_train.iloc[start_idx:].copy()
        else:
            end_idx = (i + 1) * samples_per_slice
            df_part = df_train.iloc[start_idx:end_idx].copy()
        
        y = df_part.pop(THREAT_TYPE).values
        x = df_part.values
        
        # ç¡®ä¿yä¸ºæ•´æ•°ç±»å‹ (å¤„ç†numpyæ•°ç»„)
        y = pd.Series(y)
        y = pd.to_numeric(y, errors='coerce').fillna(0).astype(int).values
        
        # æ˜¾ç¤ºèŠ‚ç‚¹æ•°æ®åˆ†å¸ƒ
        unique, counts = np.unique(y, return_counts=True)
        print(f"         èŠ‚ç‚¹{i} - æ ·æœ¬æ•°: {len(y)}, ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        # SMOTEæ•°æ®å¹³è¡¡
        if isSmote:
            sm = SMOTE(random_state=42)
            try:
                x, y = sm.fit_resample(x, y)
                unique_after, counts_after = np.unique(y, return_counts=True)
                print(f"         èŠ‚ç‚¹{i} SMOTEå - æ ·æœ¬æ•°: {len(y)}, ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_after, counts_after))}")
            except ValueError as e:
                print(f"âš ï¸  èŠ‚ç‚¹ {i} SMOTEè­¦å‘Š: {e}ï¼Œè·³è¿‡SMOTEå¤„ç†")
        
        x = torch.tensor(x).float()
        y = torch.tensor(y.astype('int')).type(torch.LongTensor)
        
        x_data_dict.update({xname: x})
        y_data_dict.update({yname: y})
    
    return x_data_dict, y_data_dict

# ============================================================================
# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# ============================================================================
def train(model, train_loader, criterion, optimizer, show_progress=False):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0
    
    iterator = tqdm(train_loader, desc='Training', leave=False) if show_progress else train_loader
    
    for data, target in iterator:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        if show_progress:
            iterator.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})
    
    accuracy = correct / total
    avg_loss = train_loss / len(train_loader)
    return avg_loss, accuracy

def validation(model, test_loader, criterion):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            test_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
    
    accuracy = correct / total
    avg_loss = test_loss / len(test_loader)
    return avg_loss, accuracy

def confusion_mat(model, test_loader):
    """è®¡ç®—æ··æ·†çŸ©é˜µ"""
    model.eval()
    y_true = []
    y_pred = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            output = model(data)
            _, predicted = output.max(1)
            
            y_true.extend(target.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    return confusion_matrix(y_true, y_pred)

# ============================================================================
# è”é‚¦å­¦ä¹ æ ¸å¿ƒå‡½æ•°
# ============================================================================
def create_model_optimizer_criterion_dict(number_of_slices, inputs, outputs, learning_rate, momentum):
    """ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºæ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°"""
    model_dict = dict()
    optimizer_dict = dict()
    criterion_dict = dict()

    for i in range(number_of_slices):
        model_name = "model" + str(i)
        model_info = Net2nn(inputs, outputs).to(device)
        model_dict.update({model_name: model_info})

        optimizer_name = "optimizer" + str(i)
        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)
        optimizer_dict.update({optimizer_name: optimizer_info})

        criterion_name = "criterion" + str(i)
        criterion_info = nn.CrossEntropyLoss()
        criterion_dict.update({criterion_name: criterion_info})

    return model_dict, optimizer_dict, criterion_dict

def FedAvg(w):
    """è”é‚¦å¹³å‡ç®—æ³•"""
    w_avg = copy.deepcopy(w[0])
    for k in w_avg.keys():
        for i in range(1, len(w)):
            w_avg[k] += w[i][k]
        w_avg[k] = torch.div(w_avg[k], len(w))
    return w_avg

def train_model_one_iteration(model_dict, optimizer_dict, criterion_dict, x_train_dict,
                              y_train_dict, number_of_slices, batch_size, numEpoch):
    """è®­ç»ƒæ‰€æœ‰èŠ‚ç‚¹æ¨¡å‹ä¸€æ¬¡è¿­ä»£"""
    for i in range(number_of_slices):
        model_name = "model" + str(i)
        model = model_dict[model_name]
        
        optimizer_name = "optimizer" + str(i)
        optimizer = optimizer_dict[optimizer_name]
        
        criterion_name = "criterion" + str(i)
        criterion = criterion_dict[criterion_name]
        
        x_name = "x_train" + str(i)
        x_train = x_train_dict[x_name]
        
        y_name = "y_train" + str(i)
        y_train = y_train_dict[y_name]
        
        train_ds = TensorDataset(x_train, y_train)
        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        
        print(f"\n      ğŸ”¹ èŠ‚ç‚¹ {i+1}/{number_of_slices} è®­ç»ƒä¸­...")
        epoch_pbar = tqdm(range(numEpoch), desc=f'      èŠ‚ç‚¹{i+1}', ncols=100, leave=True)
        for epoch in epoch_pbar:
            loss, acc = train(model, train_dl, criterion, optimizer, show_progress=False)
            epoch_pbar.set_postfix({'loss': f'{loss:.4f}', 'acc': f'{acc:.4f}'})

# ============================================================================
# ä¸»è®­ç»ƒæµç¨‹
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ¯ å¼€å§‹è®­ç»ƒæµç¨‹")
print("=" * 80)

# å‡†å¤‡æ•°æ®
print("\nğŸ“¦ å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®...")
print(f"   æ•°æ®é›†æ€»æ ·æœ¬æ•°: {len(df_normalized)}")
print(f"   æ­£å¸¸æ ·æœ¬: {len(df_normalized[df_normalized[THREAT_TYPE]==0])}")
print(f"   æ”»å‡»æ ·æœ¬: {len(df_normalized[df_normalized[THREAT_TYPE]==1])}")
df_train, df_test = divide_train_test(df_normalized, proportion=0.1)

print(f"\n   è®­ç»ƒé›†: {len(df_train)} æ ·æœ¬")
print(f"   æµ‹è¯•é›†: {len(df_test)} æ ·æœ¬")

x_train_dict, y_train_dict = get_data_for_slices(df_train, number_of_slices, isSmote)

y_test = df_test.pop(THREAT_TYPE).values
x_test = df_test.values

x_test = torch.tensor(x_test).float()
y_test = torch.tensor(y_test.astype('int')).type(torch.LongTensor)

inputs = x_test.shape[1]
outputs = 2  # äºŒåˆ†ç±»: æ­£å¸¸(0) vs æ”»å‡»(1)

print(f"   âœ“ è¾“å…¥ç‰¹å¾æ•°: {inputs}")
print(f"   âœ“ è¾“å‡ºç±»åˆ«æ•°: {outputs}")

# åˆ›å»ºä¸»æ¨¡å‹
print("\nğŸ—ï¸  åˆ›å»ºä¸»æ¨¡å‹...")
main_model = Net2nn(inputs, outputs).to(device)
main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=momentum)
main_criterion = nn.CrossEntropyLoss()

# åˆ›å»ºèŠ‚ç‚¹æ¨¡å‹
print("ğŸ—ï¸  åˆ›å»ºèŠ‚ç‚¹æ¨¡å‹...")
model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(
    number_of_slices, inputs, outputs, learning_rate, momentum
)

# è®­ç»ƒå‰è¯„ä¼°
print("\nğŸ“Š è®­ç»ƒå‰æ¨¡å‹æ€§èƒ½...")
test_ds = TensorDataset(x_test, y_test)
test_dl = DataLoader(test_ds, batch_size=batch_size * 2)
before_loss, before_acc = validation(main_model, test_dl, main_criterion)
print(f"   ä¸»æ¨¡å‹å‡†ç¡®ç‡: {before_acc:.4f}")

# è”é‚¦å­¦ä¹ è®­ç»ƒ
print("\nğŸ”„ å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ...")
train_start_time = time.time()

num_iterations = 10  # è”é‚¦å­¦ä¹ è¿­ä»£æ¬¡æ•°ï¼ˆå¢åŠ åˆ°10æ¬¡ä»¥æå‡æ€§èƒ½ï¼‰

print(f"\n{'='*80}")
print(f"ğŸ“¡ è”é‚¦å­¦ä¹ è®­ç»ƒè¿›åº¦")
print(f"{'='*80}")

iteration_pbar = tqdm(range(num_iterations), desc='è”é‚¦å­¦ä¹ è¿­ä»£', ncols=100, position=0)

for iteration in iteration_pbar:
    iteration_pbar.set_description(f'ğŸ“¡ è¿­ä»£ {iteration+1}/{num_iterations}')
    
    # è®­ç»ƒæ‰€æœ‰èŠ‚ç‚¹
    train_model_one_iteration(model_dict, optimizer_dict, criterion_dict,
                             x_train_dict, y_train_dict, number_of_slices,
                             batch_size, numEpoch)
    
    # æ”¶é›†æ¨¡å‹æƒé‡
    w = []
    for i in range(number_of_slices):
        model_name = "model" + str(i)
        w.append(copy.deepcopy(model_dict[model_name].state_dict()))
    
    # è”é‚¦å¹³å‡
    w_avg = FedAvg(w)
    
    # æ›´æ–°ä¸»æ¨¡å‹
    main_model.load_state_dict(w_avg)
    
    # è¯„ä¼°ä¸»æ¨¡å‹
    loss, acc = validation(main_model, test_dl, main_criterion)
    iteration_pbar.set_postfix({'accuracy': f'{acc:.4f}', 'loss': f'{loss:.4f}'})
    print(f"\n   âœ… è¿­ä»£ {iteration+1} å®Œæˆ - ä¸»æ¨¡å‹å‡†ç¡®ç‡: {acc:.4f}")

train_time = time.time() - train_start_time
print(f"\nâ±ï¸  æ€»è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’ ({train_time/60:.2f}åˆ†é’Ÿ)")

# æœ€ç»ˆè¯„ä¼°
print("\n" + "=" * 80)
print("ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°")
print("=" * 80)

final_loss, final_acc = validation(main_model, test_dl, main_criterion)
print(f"\nğŸ¯ æœ€ç»ˆå‡†ç¡®ç‡: {final_acc:.4f} ({final_acc*100:.2f}%)")

# æ··æ·†çŸ©é˜µ
cm = confusion_mat(main_model, test_dl)
print(f"\næ··æ·†çŸ©é˜µ:")
print(cm)

# è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
if outputs == 2:
    y_true = []
    y_pred = []
    with torch.no_grad():
        for data, target in test_dl:
            data = data.to(device)
            output = main_model(data)
            _, predicted = output.max(1)
            y_true.extend(target.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    precision = precision_score(y_true, y_pred, average='binary')
    recall = recall_score(y_true, y_pred, average='binary')
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
    print(f"   ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"   å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"   F1åˆ†æ•°: {f1:.4f}")

# ============================================================================
# ä¿å­˜æ¨¡å‹
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
print("=" * 80)

# ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®
save_dict = {
    'model_state_dict': main_model.state_dict(),
    'max_values': max_values.to_dict(),
    'numerical_columns': list(max_values.index),
    'inputs': inputs,
    'outputs': outputs,
    'accuracy': final_acc,
    'training_config': {
        'learning_rate': learning_rate,
        'numEpoch': numEpoch,
        'batch_size': batch_size,
        'momentum': momentum,
        'number_of_slices': number_of_slices,
        'isSmote': isSmote,
        'num_iterations': num_iterations
    },
    'timestamp': timestamp,
    'device': str(device)
}

torch.save(save_dict, model_save_path)
print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")

# ä¿å­˜è®­ç»ƒç»“æœ
results = {
    'timestamp': timestamp,
    'final_accuracy': float(final_acc),
    'final_loss': float(final_loss),
    'training_time_seconds': train_time,
    'confusion_matrix': cm.tolist(),
    'config': save_dict['training_config']
}

if outputs == 2:
    results['precision'] = float(precision)
    results['recall'] = float(recall)
    results['f1_score'] = float(f1)

with open(results_save_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… è®­ç»ƒç»“æœå·²ä¿å­˜: {results_save_path}")

print("\n" + "=" * 80)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
print("=" * 80)
print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
print(f"   1. æ¨¡å‹æ–‡ä»¶: {model_save_path}")
print(f"   2. ç»“æœæ–‡ä»¶: {results_save_path}")
print(f"\nğŸ’¡ ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ£€æµ‹:")
print(f"   from intrusion_detection_system.intrusion_detector import IntrusionDetector")
print(f"   detector = IntrusionDetector('{model_save_path}')")
print("=" * 80)
